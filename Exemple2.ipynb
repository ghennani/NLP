{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla | is | looking | at | buying | U.S. | startup | for | $ | 69 | million | "
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Tesla is looking at buying U.S. startup for $69 million')\n",
    "for token in doc:\n",
    "    print(token.text,end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import  word_tokenize\n",
    "EXAMPLE_TEXT = \"\"\"\n",
    "Hello Mr. Smith, how are you doing today? The weather is great, \n",
    "and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\n",
    "\"\"\"\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "يعد\n",
      "الذكاء\n",
      "الإصطناعي\n",
      "من\n",
      "العلوم\n",
      "التي\n",
      "يتسارع\n",
      "التطور\n",
      "فيها\n",
      "بشكل\n",
      "لافت\n",
      "منذ\n",
      "عام\n",
      "2005\n",
      "و\n",
      "لمدة\n",
      "15\n",
      "سنة\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(' يعد الذكاء الإصطناعي من العلوم التي يتسارع التطور فيها بشكل لافت منذ عام 2005 و لمدة 15 سنة ')\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "( , يعد, الذكاء)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[0] , doc[1] , doc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يمكنك\n",
      "مراسلتنا\n",
      "علي\n",
      "البريد\n",
      "الإلكتروني\n",
      "للشركة\n",
      "هو\n",
      "info@hp.com\n",
      "  \n",
      "او\n",
      "تصفح\n",
      "موقع\n",
      "الشركة\n",
      "وهو\n",
      "www.hp.com\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(u\"يمكنك مراسلتنا علي البريد الإلكتروني للشركة هو info@hp.com   او تصفح موقع الشركة وهو www.hp.com \")\n",
    "\n",
    "for token in doc4:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['يمكنك',\n",
       " 'مراسلتنا',\n",
       " 'علي',\n",
       " 'البريد',\n",
       " 'الإلكتروني',\n",
       " 'للشركة',\n",
       " 'هو',\n",
       " 'info',\n",
       " '@',\n",
       " 'hp.com',\n",
       " 'او',\n",
       " 'تصفح',\n",
       " 'موقع',\n",
       " 'الشركة',\n",
       " 'وهو',\n",
       " 'www.hp.com']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc4 = \"يمكنك مراسلتنا علي البريد الإلكتروني للشركة هو info@hp.com   او تصفح موقع الشركة وهو www.hp.com \"\n",
    "word_tokenize(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | "
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
    "for token in doc1:\n",
    "    print(token.text, end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple  :  ORG  :  Companies, agencies, institutions, etc.\n",
      "Hong Kong  :  GPE  :  Countries, cities, states\n",
      "$6 million  :  MONEY  :  Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for token in doc1.ents:\n",
    "    print(token.text , ' : ' ,  token.label_ , ' : ' , str(spacy.explain(token.label_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run-->run\n",
      "runner-->runner\n",
      "running-->run\n",
      "ran-->ran\n",
      "runs-->run\n",
      "easily-->easili\n",
      "fairly-->fairli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "p_stemmer =PorterStemmer()\n",
    "words=['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+'-->'+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple      NOUN     NN     noun, singular or mass\n",
      "to         PART     TO     infinitival \"to\"\n",
      "build      VERB     VB     verb, base form\n",
      "a          DET      DT     determiner\n",
      "Hong       PROPN    NNP    noun, proper singular\n",
      "Kong       PROPN    NNP    noun, proper singular\n",
      "factory    NOUN     NN     noun, singular or mass\n",
      "for        ADP      IN     conjunction, subordinating or preposition\n",
      "$          SYM      $      symbol, currency\n",
      "6          NUM      CD     cardinal number\n",
      "million    NUM      CD     cardinal number\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(f'{token.text:{10}} {token.pos_:{8}} {token.tag_:{6}} {spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85. ADP  : 1\n",
      "90. DET  : 1\n",
      "92. NOUN : 2\n",
      "93. NUM  : 2\n",
      "94. PART : 1\n",
      "96. PROPN: 2\n",
      "99. SYM  : 1\n",
      "100. VERB : 1\n"
     ]
    }
   ],
   "source": [
    "POS_counts = doc1.count_by(spacy.attrs.POS)\n",
    "for k,v in sorted(POS_counts.items()):\n",
    "    print(f'{k}. {doc1.vocab[k].text:{5}}: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThomas Gradgrind, sir.',\n",
       " 'A man of realities.',\n",
       " 'A man of facts and calculations.',\n",
       " 'A man who proceeds upon the principle that\\ntwo and two are four, and nothing over, and who is not to be talked into allowing for anything over.',\n",
       " 'Thomas Gradgrind, \\nsir—peremptorily Thomas—Thomas Gradgrind.',\n",
       " 'With a rule and a pair of scales, and the multiplication table always in his pocket, \\nsir, ready to weigh and measure any parcel of human nature, and tell you exactly what it comes to.',\n",
       " 'It is a mere question of\\nfigures, a case of simple arithmetic.',\n",
       " 'You might hope to get some other nonsensical belief into the head of George Gradgrind, or Augustus Gradgrind, or John Gradgrind, or Joseph Gradgrind (all supposititious, non-existent persons), but into the head of Thomas Gradgrind—no, sir!',\n",
       " 'In such terms Mr.',\n",
       " 'Gradgrind always mentally introduced himself, whether to his private circle of acquaintance, or to the public in general.',\n",
       " 'In such terms, no doubt, substituting the words ‘boys and girls,’ for ‘sir,’ Thomas Gradgrind now presented Thomas Gradgrind to the little pitchers before him, who were to be filled so full of facts.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "text = '''\n",
    "Thomas Gradgrind, sir. A man of realities. A man of facts and calculations. A man who proceeds upon the principle that\n",
    "two and two are four, and nothing over, and who is not to be talked into allowing for anything over. Thomas Gradgrind, \n",
    "sir—peremptorily Thomas—Thomas Gradgrind. With a rule and a pair of scales, and the multiplication table always in his pocket, \n",
    "sir, ready to weigh and measure any parcel of human nature, and tell you exactly what it comes to. It is a mere question of\n",
    "figures, a case of simple arithmetic. You might hope to get some other nonsensical belief into the head of George Gradgrind, or Augustus Gradgrind, or John Gradgrind, or Joseph Gradgrind (all supposititious, non-existent persons), but into the head of Thomas Gradgrind—no, sir!\n",
    "\n",
    "In such terms Mr. Gradgrind always mentally introduced himself, whether to his private circle of acquaintance, or to the public in general. In such terms, no doubt, substituting the words ‘boys and girls,’ for ‘sir,’ Thomas Gradgrind now presented Thomas Gradgrind to the little pitchers before him, who were to be filled so full of facts.\n",
    "'''\n",
    "sentencetokenizer = PunktSentenceTokenizer()\n",
    "sentencetokenizer.train(text)\n",
    "tokens = sentencetokenizer.tokenize(text)\n",
    "\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas\n",
      "Gradgrind\n",
      ",\n",
      "sir\n",
      ".\n",
      "A\n",
      "man\n",
      "realities\n",
      ".\n",
      "A\n",
      "man\n",
      "facts\n",
      "calculations\n",
      ".\n",
      "A\n",
      "man\n",
      "proceeds\n",
      "upon\n",
      "principle\n",
      "two\n",
      "two\n",
      "four\n",
      ",\n",
      "nothing\n",
      ",\n",
      "talked\n",
      "allowing\n",
      "anything\n",
      ".\n",
      "Thomas\n",
      "Gradgrind\n",
      ",\n",
      "sir—peremptorily\n",
      "Thomas—Thomas\n",
      "Gradgrind\n",
      ".\n",
      "With\n",
      "rule\n",
      "pair\n",
      "scales\n",
      ",\n",
      "multiplication\n",
      "table\n",
      "always\n",
      "pocket\n",
      ",\n",
      "sir\n",
      ",\n",
      "ready\n",
      "weigh\n",
      "measure\n",
      "parcel\n",
      "human\n",
      "nature\n",
      ",\n",
      "tell\n",
      "exactly\n",
      "comes\n",
      ".\n",
      "It\n",
      "mere\n",
      "question\n",
      "figures\n",
      ",\n",
      "case\n",
      "simple\n",
      "arithmetic\n",
      ".\n",
      "You\n",
      "might\n",
      "hope\n",
      "get\n",
      "nonsensical\n",
      "belief\n",
      "head\n",
      "George\n",
      "Gradgrind\n",
      ",\n",
      "Augustus\n",
      "Gradgrind\n",
      ",\n",
      "John\n",
      "Gradgrind\n",
      ",\n",
      "Joseph\n",
      "Gradgrind\n",
      "(\n",
      "supposititious\n",
      ",\n",
      "non-existent\n",
      "persons\n",
      ")\n",
      ",\n",
      "head\n",
      "Thomas\n",
      "Gradgrind—no\n",
      ",\n",
      "sir\n",
      "!\n",
      "In\n",
      "terms\n",
      "Mr.\n",
      "Gradgrind\n",
      "always\n",
      "mentally\n",
      "introduced\n",
      ",\n",
      "whether\n",
      "private\n",
      "circle\n",
      "acquaintance\n",
      ",\n",
      "public\n",
      "general\n",
      ".\n",
      "In\n",
      "terms\n",
      ",\n",
      "doubt\n",
      ",\n",
      "substituting\n",
      "words\n",
      "‘\n",
      "boys\n",
      "girls\n",
      ",\n",
      "’\n",
      "‘\n",
      "sir\n",
      ",\n",
      "’\n",
      "Thomas\n",
      "Gradgrind\n",
      "presented\n",
      "Thomas\n",
      "Gradgrind\n",
      "little\n",
      "pitchers\n",
      ",\n",
      "filled\n",
      "full\n",
      "facts\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = '''\n",
    "Thomas Gradgrind, sir. A man of realities. A man of facts and calculations. A man who proceeds upon the principle that\n",
    "two and two are four, and nothing over, and who is not to be talked into allowing for anything over. Thomas Gradgrind, \n",
    "sir—peremptorily Thomas—Thomas Gradgrind. With a rule and a pair of scales, and the multiplication table always in his pocket, \n",
    "sir, ready to weigh and measure any parcel of human nature, and tell you exactly what it comes to. It is a mere question of\n",
    "figures, a case of simple arithmetic. You might hope to get some other nonsensical belief into the head of George Gradgrind, or Augustus Gradgrind, or John Gradgrind, or Joseph Gradgrind (all supposititious, non-existent persons), but into the head of Thomas Gradgrind—no, sir!\n",
    "\n",
    "In such terms Mr. Gradgrind always mentally introduced himself, whether to his private circle of acquaintance, or to the public in general. In such terms, no doubt, substituting the words ‘boys and girls,’ for ‘sir,’ Thomas Gradgrind now presented Thomas Gradgrind to the little pitchers before him, who were to be filled so full of facts.\n",
    "'''\n",
    "#stops=nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#tokenize the text\n",
    "words=nltk.word_tokenize(text)\n",
    "# Get the stopwords corpus\n",
    "stops = set(stopwords.words('english'))\n",
    "# Remove the stopwords from the words list\n",
    "filtered_words = [word for word in words if not word in stops]\n",
    "\n",
    "for word in filtered_words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in the text:\n",
      "of\n",
      "of\n",
      "and\n",
      "who\n",
      "the\n",
      "that\n",
      "and\n",
      "are\n",
      "and\n",
      "over\n",
      "and\n",
      "who\n",
      "is\n",
      "not\n",
      "to\n",
      "be\n",
      "into\n",
      "for\n",
      "over\n",
      "a\n",
      "and\n",
      "a\n",
      "of\n",
      "and\n",
      "the\n",
      "in\n",
      "his\n",
      "to\n",
      "and\n",
      "any\n",
      "of\n",
      "and\n",
      "you\n",
      "what\n",
      "it\n",
      "to\n",
      "is\n",
      "a\n",
      "of\n",
      "a\n",
      "of\n",
      "to\n",
      "some\n",
      "other\n",
      "into\n",
      "the\n",
      "of\n",
      "or\n",
      "or\n",
      "or\n",
      "all\n",
      "but\n",
      "into\n",
      "the\n",
      "of\n",
      "such\n",
      "himself\n",
      "to\n",
      "his\n",
      "of\n",
      "or\n",
      "to\n",
      "the\n",
      "in\n",
      "such\n",
      "no\n",
      "the\n",
      "and\n",
      "for\n",
      "now\n",
      "to\n",
      "the\n",
      "before\n",
      "him\n",
      "who\n",
      "were\n",
      "to\n",
      "be\n",
      "so\n",
      "of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\HP 450\n",
      "[nltk_data]     G8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP 450\n",
      "[nltk_data]     G8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = '''\n",
    "Thomas Gradgrind, sir. A man of realities. A man of facts and calculations. A man who proceeds upon the principle that\n",
    "two and two are four, and nothing over, and who is not to be talked into allowing for anything over. Thomas Gradgrind, \n",
    "sir—peremptorily Thomas—Thomas Gradgrind. With a rule and a pair of scales, and the multiplication table always in his pocket, \n",
    "sir, ready to weigh and measure any parcel of human nature, and tell you exactly what it comes to. It is a mere question of\n",
    "figures, a case of simple arithmetic. You might hope to get some other nonsensical belief into the head of George Gradgrind, or Augustus Gradgrind, or John Gradgrind, or Joseph Gradgrind (all supposititious, non-existent persons), but into the head of Thomas Gradgrind—no, sir!\n",
    "\n",
    "In such terms Mr. Gradgrind always mentally introduced himself, whether to his private circle of acquaintance, or to the public in general. In such terms, no doubt, substituting the words ‘boys and girls,’ for ‘sir,’ Thomas Gradgrind now presented Thomas Gradgrind to the little pitchers before him, who were to be filled so full of facts.\n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# tokenize the text\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# get the stopwords corpus\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stopwords in the text\n",
    "print(\"Stopwords in the text:\")\n",
    "for word in words:\n",
    "    if word in stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in the text:\n",
      "من\n",
      "التي\n",
      "فيها\n",
      "منذ\n",
      "و\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\HP 450\n",
      "[nltk_data]     G8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP 450\n",
      "[nltk_data]     G8\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = '''\n",
    " يعد الذكاء الإصطناعي من العلوم التي يتسارع التطور فيها بشكل لافت منذ عام 2005 و لمدة 15 سنة \n",
    "'''\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# tokenize the text\n",
    "words = nltk.word_tokenize(text)\n",
    "\n",
    "# get the stopwords corpus\n",
    "stops = set(stopwords.words('arabic'))\n",
    "\n",
    "# Print the stopwords in the text\n",
    "print(\"Stopwords in the text:\")\n",
    "for word in words:\n",
    "    if word in stops:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يعد\n",
      "الذكاء\n",
      "الإصطناعي\n",
      "العلوم\n",
      "يتسارع\n",
      "التطور\n",
      "بشكل\n",
      "لافت\n",
      "عام\n",
      "2005\n",
      "لمدة\n",
      "15\n",
      "سنة\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in words if not word in stops]\n",
    "\n",
    "for word in filtered_words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ID 15819748337290189210 , starts at 2 & ends at 3 , and word is الذكاءالإصطناعي\n",
      "Word ID 15819748337290189210 , starts at 18 & ends at 20 , and word is الذكاء الإصطناعي\n",
      "Word ID 15819748337290189210 , starts at 26 & ends at 28 , and word is الذكاء الصناعي\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{'LOWER': 'الذكاءالإصطناعي'}]\n",
    "pattern2 = [{'LOWER': 'الذكاء'}, {'LOWER': 'الإصطناعي'}]\n",
    "pattern3 = [{'LOWER': 'الذكاء'},  {'LOWER': 'الصناعي'}]\n",
    "\n",
    "matcher.add('الذكاء الإصطناعي', None, pattern1, pattern2, pattern3)\n",
    "document = nlp('''\n",
    "يتميز الذكاءالإصطناعي أنه يسير بسرعة كبيرة نحو المستقبل\n",
    "\n",
    "و قد بدأ الكثير من الطلاب في دراسة الذكاء الإصطناعي\n",
    "\n",
    "و تزداد فرص العمل في الذكاء الصناعي في كل مكان\n",
    "\n",
    "''')\n",
    "\n",
    "found_matches = matcher(document)\n",
    "for a,b,c in  found_matches : \n",
    "    print(f'Word ID {a} , starts at {b} & ends at {c} , and word is {doc[b:c]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studysession",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
